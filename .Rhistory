#---    1)  a string vector of keywords     (default: covid19)
#---    2)  a state abbreviation            (default: whole US)
#---    2)  a boolean of percentage change  (default: FALSE)
my.fun <- function(vector.keyword = "covid19",  state.abbr = "US", change = FALSE){
library(gtrendsR)
library(tidyverse)
library(dplyr)
result <- gtrends(keyword = vector.keyword, geo = as.character(state.abbr), time = "2020-03-01 2020-04-15",
gprop = c("web"),
category = 0, hl = "en-US", low_search_volume = FALSE,
cookie_url = "http://trends.google.com/Cookies/NID", tz = 0,
onlyInterest = T)
result <- as.data.frame(result[1]) %>%
mutate(interest_over_time.hits.change = interest_over_time.hits - lag(interest_over_time.hits))
lockdown <- read.csv("https://raw.githubusercontent.com/williamloh1/teambeta/master/Project/datasets/lockdown_dates.csv")
USlockdown <- lockdown[which(lockdown$Country=="United States"), ]
USlockdown$Place <- state.abb[match(USlockdown$Place, state.name)]
USlockdown$Place <- paste0("US-", USlockdown$Place)
if(change == FALSE){if(is.element(state.abbr, USlockdown$Place)==T){
statelock <- USlockdown[USlockdown$Place==state.abbr, ]
ggplot(result, aes(x=as.Date(interest_over_time.date), y=interest_over_time.hits)) +
geom_line( color="#69b3a2") +
ylab("Hits") +
ggtitle(paste0("Search trend of ", vector.keyword, " over time")) +
xlab("") +
ylim(0,100) +
theme_light() +
theme(axis.text.x=element_text(angle=60, hjust=1))+
geom_vline(xintercept = as.Date(statelock$Start.date))+
geom_text(aes(x=as.Date(statelock$Start.date), label="\nafter lockdown", y=mean(range(interest_over_time.hits))), size=4, colour="grey", angle=90) +
geom_text(aes(x=as.Date(statelock$Start.date), label="before lockdown\n", y=mean(range(interest_over_time.hits))), size=4, colour="grey", angle=90)
} else{ggplot(result, aes(x=as.Date(interest_over_time.date), y=interest_over_time.hits)) +
geom_line( color="#69b3a2") +
ylab("Hits") +
ggtitle(paste0("Search trend of ", vector.keyword, " over time")) +
xlab("") +
ylim(0,100) +
theme_light() +
theme(axis.text.x=element_text(angle=60, hjust=1))}
} else {if(is.element(state.abbr, USlockdown$Place)==T){
statelock <- USlockdown[USlockdown$Place==state.abbr, ]
ggplot(result, aes(x=as.Date(interest_over_time.date), y=interest_over_time.hits.change)) +
geom_line( color="#69b3a2") +
ylab("Change in Hits") +
ggtitle(paste0("Change of search trend of ", vector.keyword, " over time")) +
xlab("") +
theme_light() +
theme(axis.text.x=element_text(angle=60, hjust=1))+
geom_vline(xintercept = as.Date(statelock$Start.date))+
geom_text(aes(x=as.Date(statelock$Start.date), label="\nafter lockdown", y=mean(range(interest_over_time.hits.change[-1]))), size=4, colour="grey", angle=90) +
geom_text(aes(x=as.Date(statelock$Start.date), label="before lockdown\n", y=mean(range(interest_over_time.hits.change[-1]))), size=4, colour="grey", angle=90)
} else{
ggplot(result, aes(x=as.Date(interest_over_time.date), y=interest_over_time.hits.change)) +
geom_line( color="#69b3a2") +
ylab("Change in Hits") +
ggtitle(paste0("Change of search trend of ", vector.keyword, " over time")) +
xlab("") +
theme_light() +
theme(axis.text.x=element_text(angle=60, hjust=1))
}
}
}
statecodes <- read.csv("https://raw.githubusercontent.com/williamloh1/teambeta/master/Project/datasets/state%20codes.csv")
shinyApp(
ui = fluidPage(
sidebarLayout( ## Choosing layout with inputs on side and
## outputs displayed in the main body
sidebarPanel( #Things in this function specify the sidebar
selectInput(label = "State: ",
choices = statecodes$state, inputId = "stateterm"),
textInput(inputId = "searchterm",
label = "Search Term:",
value=""
),
selectInput(label = "Type of query: ",
choices = c("Hits per Day","Day-to-day change in hits"),
inputId = "querytype")),
## End of sidebar
mainPanel( ## Arguments for main section (output)
plotOutput("searchGraph")
) # Close main panel
)
),
server = function(input, output){
output$searchGraph <- renderPlot({
statecodes <- read.csv("https://raw.githubusercontent.com/williamloh1/teambeta/master/Project/datasets/state%20codes.csv")
state.code <- statecodes$state_code[statecodes$state == input$stateterm]
bool <- T
if (input$querytype == "Hits per Day") {
bool <- F
}
graph.output <- my.fun(vector.keyword = input$searchterm, state.abbr = paste0("US-", state.code), bool)
print(graph.output)
})
},
options = list(height = 500)
)
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.pos = 'H')
library(car)
rm(list = ls())
library(car)
communist_data <- read.csv("G:/My Drive/_WashU_courses/2020_Spring/QPM/Assignments/W9/communities.csv", header=TRUE)
vif.model1 <- vif(model1 <- lm(ViolentCrimesPerPop ~ . -racepctblack - agePct12t29
-medIncome - pctWWage -PctPopUnderPov -MalePctNevMarr -PctIlleg
-RentLowQ -NumInShelters, communist_data))
vif(model1 <- lm(ViolentCrimesPerPop ~ . -racepctblack - agePct12t29
-medIncome - pctWWage -PctPopUnderPov -MalePctNevMarr -PctIlleg
-RentLowQ -NumInShelters, communist_data))
vif.model1[vif.model1>=2]
vif.model1[vif.model1>=4]
vif.model1[vif.model1>=2]
vif.model1[sqrt(vif.model1)>=2]
model1
corr(population,racePctWhite,racePctAsian,racePctHisp,agePct16t24,agePct65up,pctUrban,perCapInc,PctLess9thGrade,PctUnemployed,PctWOFullPlumb,   MedRentPctHousInc,NumStreet,PopDensPctUsePubTrans,LemasPctOfficDrugUn)
with(communist_data, corr(population,racePctWhite,racePctAsian,racePctHisp,agePct16t24,agePct65up,pctUrban,perCapInc,PctLess9thGrade,PctUnemployed,PctWOFullPlumb,   MedRentPctHousInc,NumStreet,PopDensPctUsePubTrans,LemasPctOfficDrugUn))
with(communist_data, cor(population,racePctWhite,racePctAsian,racePctHisp,agePct16t24,agePct65up,pctUrban,perCapInc,PctLess9thGrade,PctUnemployed,PctWOFullPlumb,   MedRentPctHousInc,NumStreet,PopDensPctUsePubTrans,LemasPctOfficDrugUn))
cor(communist_data[, c("population","racePctWhite","racePctAsian","racePctHisp","agePct16t24","agePct65up","pctUrban","perCapInc","PctLess9thGrade","PctUnemployed","PctWOFullPlumb","MedRentPctHousInc","NumStreet","PopDensPctUsePubTrans","LemasPctOfficDrugUn"]))
cor(communist_data[, c("population","racePctWhite","racePctAsian","racePctHisp","agePct16t24","agePct65up","pctUrban","perCapInc","PctLess9thGrade","PctUnemployed","PctWOFullPlumb","MedRentPctHousInc","NumStreet","PopDensPctUsePubTrans","LemasPctOfficDrugUn"]))
cor(communist_data[, c("population","racePctWhite","racePctAsian","racePctHisp","agePct16t24","agePct65up","pctUrban","perCapInc","PctLess9thGrade","PctUnemployed","PctWOFullPlumb","MedRentPctHousInc","NumStreet","PopDensPctUsePubTrans","LemasPctOfficDrugUn")])
communist_data[, c("population","racePctWhite","racePctAsian","racePctHisp","agePct16t24","agePct65up","pctUrban","perCapInc","PctLess9thGrade","PctUnemployed","PctWOFullPlumb","MedRentPctHousInc","NumStreet","PopDensPctUsePubTrans","LemasPctOfficDrugUn")]
communist_data[, c("population","racePctWhite","racePctAsian","racePctHisp","agePct16t24","agePct65up","pctUrban","perCapInc","PctLess9thGrade","PctUnemployed","PctWOFullPlumb","MedRentPctHousInc","NumStreet","PopDensPctUsePubTrans","LemasPctOfficDrugUn")]
communist_data[, c("population")]
communist_data[, c("population","racePctWhite")]
cor(communist_data[, c("population","racePctWhite","racePctAsian","racePctHisp","agePct16t24","agePct65up","pctUrban","perCapInc","PctLess9thGrade","PctUnemployed","PctWOFullPlumb","MedRentPctHousInc","NumStreet","PopDensPctUsePubTrans","LemasPctOfficDrugUn")])
t
cor(communist_data[, c("population","racePctWhite","racePctAsian","racePctHisp","agePct16t24",
"agePct65up","pctUrban","perCapInc","PctLess9thGrade","PctUnemployed",
"PctWOFullPlumb","MedRentPctHousInc","NumStreet","PopDensPctUsePubTrans",
"LemasPctOfficDrugUn")])
corr(communist_data)
cor(communist_data)
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.pos = 'H')
rm(list = ls())
communist_data <- read.csv("G:/My Drive/_WashU_courses/2020_Spring/QPM/Assignments/W9/communities.csv", header=TRUE)
##  Let's look at the OLS coefficients
library("dplyr")
com_train <- sample_n(communist_data, size = 1500, replace = FALSE)
com_train_resp <- com_train[,1]
com_train_pred <- as.matrix(com_train[,2:26])
##  LINEAR Regression MODEL
model_linear <- lm(ViolentCrimesPerPop ~ . ,com_train)
##  Ridge Regression MODEL
plot(lm.ridge(ViolentCrimesPerPop ~ ., data = com_train, lambda = seq(-75, 5, 10)))
##  Let's look at the OLS coefficients
library("dplyr")
##  Ridge Regression MODEL
plot(lm.ridge(ViolentCrimesPerPop ~ ., data = com_train, lambda = seq(-75, 5, 10)))
library(glmnet)  # for ridge regression
library(dplyr)   # for data cleaning
##install.packages("psych")
library(psych)   # for function tr() to compute trace of a matrix
##  Ridge Regression MODEL
plot(lm.ridge(ViolentCrimesPerPop ~ ., data = com_train, lambda = seq(-75, 5, 10)))
##  Ridge Regression MODEL
plot(lm.ridge(ViolentCrimesPerPop ~ ., data = com_train, lambda = seq(-75, 5, 10)))
##  Ridge Regression MODEL
library(MASS)
plot(lm.ridge(ViolentCrimesPerPop ~ ., data = com_train, lambda = seq(-75, 5, 10)))
select(lm.ridge(ViolentCrimesPerPop ~ ., data = com_train, lambda = seq(-75, 5, 10)))
ridge_train2 <- lm.ridge(ViolentCrimesPerPop ~ ., data = com_train, lambda = seq(5))
coef(ridge_train2)
plot(lm.ridge(ViolentCrimesPerPop ~ ., data = com_train, lambda = seq(-75, 5, 10)))
## Load libraries, get data & set seed for reproducibility ---------------------
set.seed(123)    # seef for reproducibility
## Perform 10-fold cross-validation to select lambda ---------------------------
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
## Setting alpha = 0 implements ridge regression
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
model_ridge <- cv.glmnet(com_train_pred, com_train_resp, alpha = 0, lambda = lambdas_to_try,
standardize = TRUE, nfolds = 10)
## Plot cross-validation results
plot(model_ridge)
## Best cross-validated lambda
lambda_cv <- model_ridge$lambda.min
## Fit final model, get its sum of squared residuals and multiple R-squared
model_cv <- glmnet(com_train_pred, com_train_resp, alpha = 0, lambda = lambda_cv, standardize = TRUE)
##  Lasso Regression MODEL
##install.packages("lars")
library(lars)
trainY <- com_train$ViolentCrimesPerPop
trainX <- as.matrix(com_train[,2:26])
las_train <- lars(trainX, trainY, type = "lasso")
las_train
plot(las_train)
summary(las_train)
y_hat_lm <- predict(model_linear, as.data.frame(com_train_pred))
ssr_lm <- crossprod(com_train_resp - y_hat_lm)
rsq_lm <- cor(com_train_resp, y_hat_lm)^2
y_hat_cv <- predict(model_cv, com_train_pred)
ssr_cv <- crossprod(com_train_resp - y_hat_cv)
rsq_ridge_cv <- cor(com_train_resp, y_hat_cv)^2
y_hat_lasso <- com_train_pred %*% as.matrix(coef(las_train)[28,])
ssr_lasso <- crossprod(com_train_resp - y_hat_lasso)
rsq_lasss <- cor(com_train_resp, y_hat_lasso)^2
kable(data.frame(ssr_lm, rsq_lm, ssr_cv, rsq_ridge_cv, ssr_lasso, rsq_lasss, ))
(data.frame(ssr_lm, rsq_lm, ssr_cv, rsq_ridge_cv, ssr_lasso, rsq_lasss, ))
(data.frame((ssr_lm, rsq_lm, ssr_cv, rsq_ridge_cv, ssr_lasso, rsq_lasss))
(data.frame(c(ssr_lm, rsq_lm, ssr_cv, rsq_ridge_cv, ssr_lasso, rsq_lasss))
data.frame(c(ssr_lm, rsq_lm, ssr_cv, rsq_ridge_cv, ssr_lasso, rsq_lasss))
as.data.frame(c(ssr_lm, rsq_lm, ssr_cv, rsq_ridge_cv, ssr_lasso, rsq_lasss))
y_hat_lasso <- com_train_pred %*% as.matrix(coef(las_train)[28,])
ssr_lasso <- crossprod(com_train_resp - y_hat_lasso)
rsq_lasss <- cor(com_train_resp, y_hat_lasso)^2
as.data.frame(c(ssr_lm, rsq_lm, ssr_cv, rsq_ridge_cv, ssr_lasso, rsq_lasss))
coef(las_train)
coef(las_train)[28,]
as.matrix(coef(las_train)[28,])
y_hat_lasso <- com_train_pred %*% as.matrix(coef(las_train)[28,])
com_train_pred
com_train_pred %*% as.matrix(coef(las_train)[28,])
ssr_lasso <- crossprod(com_train_resp - y_hat_lasso)
rsq_lasss <- cor(com_train_resp, y_hat_lasso)^2
as.data.frame(c(ssr_lm, rsq_lm, ssr_cv, rsq_ridge_cv, ssr_lasso, rsq_lasss))
com_train_resp
_pred %*% as.matrix(coef(las_train)[28,])
com_train_resp - y_hat_lasso
com_train_resp - y_hat_cv
y_hat_lm <- predict(model_linear, as.data.frame(com_train_pred))
ssr_lm <- crossprod(com_train_resp - y_hat_lm)
rsq_lm <- cor(com_train_resp, y_hat_lm)^2
y_hat_cv <- predict(model_cv, com_train_pred)
ssr_cv <- crossprod(com_train_resp - y_hat_cv)
rsq_ridge_cv <- cor(com_train_resp, y_hat_cv)^2
y_hat_lasso <- com_train_pred %*% as.matrix(coef(las_train)[28,])
ssr_lasso <- crossprod(com_train_resp - y_hat_lasso)
rsq_lasss <- cor(com_train_resp, y_hat_lasso)^2
as.data.frame(c(ssr_lm, rsq_lm, ssr_cv, rsq_ridge_cv, ssr_lasso, rsq_lasss))
library(faraway)
library(MASS)
library(car)
library(texreg)
library(apsrtable)
library(xtable)
data(teengamb)
dim(teengamb)
summary(teengamb)
mod.gamb.1 <- lm(gamble ~ sex + status + income + verbal, data=teengamb)
summary(mod.gamb.1)
par(mfrow=c(1,2), mar=c(4,4,2,0.5)) ## to get smaller margins
qqnorm(residuals(mod.gamb.1), ylab="Residuals", pch=16)
qqline(residuals(mod.gamb.1))
plot(density(residuals(mod.gamb.1)), lwd=2, main="Density plot of the residuals", xlab="Residuals", xlim=c(-100,100), ylim=c(0, 0.03))
par(new=TRUE) ## to superimpose the new plot, rather than plotting it in another window
set.seed(5178)
plot(density(rnorm(1000, mean=0, sd=sd(residuals(mod.gamb.1)))), lwd=2, col="firebrick2", axes=FALSE, xlab="", ylab="", main="", xlim=c(-100,100), ylim=c(0, 0.03))
legend("topleft",
lwd=c(2,2),
lty=c(1,1),
col=c("black", "firebrick2"),
legend=c("residuals", "normal dist."))
plot(density(residuals(mod.gamb.1)), lwd=2, main="Density plot of the residuals", xlab="Residuals", xlim=c(-100,100), ylim=c(0, 0.03))
par(new=TRUE) ## to superimpose the new plot, rather than plotting it in another window
set.seed(5178)
plot(density(rnorm(1000, mean=0, sd=sd(residuals(mod.gamb.1)))), lwd=2, col="firebrick2", axes=FALSE, xlab="", ylab="", main="", xlim=c(-100,100), ylim=c(0, 0.03))
legend("topleft",
lwd=c(2,2),
lty=c(1,1),
col=c("black", "firebrick2"),
legend=c("residuals", "normal dist."))
plot(density(residuals(linear.model)), lwd=2, main="Density plot of the residuals", xlab="Residuals", xlim=c(-100,100), ylim=c(0, 0.03))
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.pos = 'H')
rm(list = ls())       # Getting ready:
library(faraway)      # The library with dataset
data("cornnit")       # The dataset
library(stargazer)
linear.model <- lm(yield ~ nitrogen, cornnit)    # Running our model
stargazer(linear.model,
header = FALSE, title = "OLS Model", type = 'latex')
library(car)
avPlots(linear.model)
plot(linear.model, 1)
plot(density(residuals(linear.model)), lwd=2, main="Density plot of the residuals", xlab="Residuals", xlim=c(-100,100), ylim=c(0, 0.03))
par(new=TRUE) ## to superimpose the new plot, rather than plotting it in another window
set.seed(5178)
plot(density(rnorm(1000, mean=0, sd=sd(residuals(mod.gamb.1)))), lwd=2, col="firebrick2", axes=FALSE, xlab="", ylab="", main="", xlim=c(-100,100), ylim=c(0, 0.03))
legend("topleft",
lwd=c(2,2),
lty=c(1,1),
col=c("black", "firebrick2"),
legend=c("residuals", "normal dist."))
plot(density(residuals(linear.model)), lwd=2, main="Density plot of the residuals", xlab="Residuals", xlim=c(-100,100), ylim=c(0, 0.03))
par(new=TRUE) ## to superimpose the new plot, rather than plotting it in another window
plot(density(rnorm(1000, mean=0, sd=sd(residuals(mod.gamb.1)))), lwd=2, col="firebrick2", axes=FALSE, xlab="", ylab="", main="", xlim=c(-100,100), ylim=c(0, 0.03))
plot(density(residuals(linear.model)), lwd=2, main="Density plot of the residuals", xlab="Residuals", xlim=c(-100,100), ylim=c(0, 0.03))
par(new=TRUE) ## to superimpose the new plot, rather than plotting it in another window
plot(density(rnorm(1000, mean=0, sd=sd(residuals(linear.model)))), lwd=2, col="firebrick2", axes=FALSE, xlab="", ylab="", main="", xlim=c(-100,100), ylim=c(0, 0.03))
legend("topleft",
lwd=c(2,2),
lty=c(1,1),
col=c("black", "firebrick2"),
legend=c("residuals", "normal dist."))
knitr::opts_chunk$set(echo = T, fig.pos = 'H')
stopifnot(require("knitr"))
stopifnot(require("tidyverse"))
stopifnot(require("miscTools"))
stopifnot(require("gridExtra"))
stopifnot(require("corpcor"))
stopifnot(require("stargazer"))
stopifnot(require("vcov"))
stopifnot(require("faraway"))
stopifnot(require("lme4"))
stopifnot(require("latex2exp"))
rm(list = ls())
library(faraway)
data(cornnit)
str(cornnit)
##  Let's fit the linear regression model:
cornnit_lm <- lm(yield ~ nitrogen, cornnit)
##  Let's look at the coefficient table:
library(stargazer)
stargazer(cornnit_lm, header=FALSE, type='latex')
##  The fitted line plot:
plot(cornnit$nitrogen, cornnit$yield)
abline(lm(yield ~ nitrogen, cornnit))
##  The QQ-Plot
plot(cornnit_lm)
##  Let's do the transformation:
cornnit <- cornnit %>%
mutate(nitrogen_dum = as.numeric(nitrogen != 0))
##  Let's fit the linear regression model:
cornnit_lm.dum <- lm(yield ~ nitrogen_dum, cornnit)
##  Now, let's compare the models:
stargazer(cornnit_lm, cornnit_lm.dum, header=FALSE, type='latex')
##  Let's see if residuals are normally distributed so that the new model is a better fit:
plot(cornnit_lm.dum)
##  Let's see if linearity assumption holds so that the new model is a better fit:
plot(cornnit$nitrogen_dum, cornnit$yield)
abline(lm(yield ~ nitrogen_dum, cornnit))
rm(list = ls())
library(faraway)
library(tidyverse)
data(fat)
str(fat)
##  Removing brozek and density
fat.new <- fat %>%
select(-c(brozek, density))
rm(list = ls())
library(faraway)
library(tidyverse)
data(fat)
str(fat)
##  Removing brozek and density
fat.new <- fat %>%
select(-c(brozek, density))
##  Removing brozek and density
fat.new <- fat %>%
select(c(brozek, density))
##  Removing brozek and density
fat.new <- fat %>%
select(-c("brozek", "density"))
##  Removing brozek and density
fat.new <- fat %>%
select(brozek, density))
##  Removing brozek and density
fat.new <- fat %>%
select(brozek, density)
##  Removing brozek and density
fat.new <- fat %>%
select(c(brozek, density))
##  Removing brozek and density
fat.new <- fat %>%
select(-brozek, -density)
##  Removing brozek and density
fat.new <- select(fat, -brozek, -density)
library(tidyverse)
##  Removing brozek and density
fat.new <- select(fat, -brozek, -density)
##  Removing brozek and density
fat.new <- select(fat, -brozek, -density)
##  Removing brozek and density
fat.new <- fat[, -c("brozek", "density")]
##  Removing brozek and density
fat.new <- fat[, -("brozek", "density")]
##  Removing brozek and density
fat.new <- fat[, "brozek", "density"]
##  Removing brozek and density
fat.new <- fat[, "brozek" "density"]
##  Removing brozek and density
fat.new <- fat[, "brozek"]
##  Removing brozek and density
fat.new <- fat[, -"brozek"]
##  Removing brozek and density
fat.new <- fact %>%
select(-density, -brozek)
##  Removing brozek and density
fat.new <- fat %>%
select(-density, -brozek)
##  Removing brozek and density
fat.new <- fat %>%
ungroup()
##  Removing brozek and density
fat.new <- fat %>%
ungroup() %>%
select(-density, -brozek)
##  Removing brozek and density
fat.new <- fat %>%
ungroup() %>%
select(-density, -brozek)
##  Removing brozek and density
fat.new <- fat %>%
ungroup() %>%
select(-density, -brozek)
##  Removing brozek and density
fat.new <- fat %>%
ungroup() %>%
dplyr::select(-density, -brozek)
rm(list = ls())
library(faraway)
library(tidyverse)
data(fat)
str(fat)
##  Removing brozek and density
fat.new <- fat %>%
ungroup() %>%
dplyr::select(-density, -brozek)
##  Creating Training and Test Datasets
fat.test  <- fat.new[seq(1,nrow(fat.new)) %% 10 == 0,]
fat.train <- fat.new[seq(1,nrow(fat.new)) %% 10 != 0,]
##  Linear model:
fat_lm <- lm(siri ~ . ,fat.train)
##  Use these models to predict the response in the test sample, and report on the
##    performances of the models.  You need not worry about providing standard errors
##    for the ridge and lasso regressions (we know these parameters will be biased, and
##    it's not much use to figure out the uncertainty of a biased parameter).
lm_pred <- predict(fat_lm, fat.test)
library(MASS)
##  Ridge Regression model:
fat_rr <- lm.ridge(siri ~ . ,fat.train)
##  Use these models to predict the response in the test sample, and report on the
##    performances of the models.  You need not worry about providing standard errors
##    for the ridge and lasso regressions (we know these parameters will be biased, and
##    it's not much use to figure out the uncertainty of a biased parameter).
rr_pred <- as.numeric(as.matrix(cbind(const=1, fat.test[,-1])) %*% coef(fat_rr))
##install.packages("lars")
library(lars)
regressors <- as.matrix(fat.train[,-1])
regressand <- fat.train$siri
##  Lasso Regression model:
fat_lrm <- lars(regressors, regressand, type='lasso')
##  Use these models to predict the response in the test sample, and report on the
##    performances of the models.  You need not worry about providing standard errors
##    for the ridge and lasso regressions (we know these parameters will be biased, and
##    it's not much use to figure out the uncertainty of a biased parameter).
#Picked s=8, this is up for the user to decide
lars_pred <- predict.lars(fat_lrm, fat.test[,-1], s=8, type='fit')
lars_pred_coef <- coef(fat_lrm, s=7, mode="step")
##install.packages("lars")
real.vs.predicted <- data.frame(fat.test$siri, lm_pred, abs(fat.test$siri - lm_pred), rr_pred,
abs(fat.test$siri - rr_pred), lars_pred$fit,  abs(fat.test$siri -lars_pred$fit))
names(real.vs.predicted) <- c("Real Y", "Yhat by LM", "Abs Discr LM",
"Yhat by Ridge", "Abs Discr RR", "Yhat by Larso", "Abs Discr LR" )
kable(real.vs.predicted, caption = "The Predicted Values and Their Discrepancy")
## See that most of the time, Larso regression has the minimum
##    absolute deviation from the real Y values.
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.pos = 'H')
rm(list = ls())       # Getting ready:
library(faraway)      # The library with dataset
data("cornnit")       # The dataset
library(stargazer)
linear.model <- lm(yield ~ nitrogen, cornnit)    # Running our model
stargazer(linear.model,
header = FALSE, title = "OLS Model", type = 'latex')
library(car)
avPlots(linear.model)
plot(linear.model, 1)
#######  Q-Q PLOT:
####
plot(linear.model, 2)
########  Density Plot of the Residuals
####
plot(density(residuals(linear.model)), lwd=2, main="Density plot of the residuals", xlab="Residuals", xlim=c(-100,100), ylim=c(0, 0.03))
par(new=TRUE) ## to superimpose the new plot, rather than plotting it in another window
plot(density(rnorm(1000, mean=0, sd=sd(residuals(linear.model)))), lwd=2, col="firebrick2", axes=FALSE, xlab="", ylab="", main="", xlim=c(-100,100), ylim=c(0, 0.03))
legend("topleft",
lwd=c(2,2),
lty=c(1,1),
col=c("black", "firebrick2"),
legend=c("residuals", "normal dist."))
rm(list = ls())
library(car)
communist_data <- read.csv("G:/My Drive/_WashU_courses/2020_Spring/QPM/Assignments/W9/communities.csv", header=TRUE)
vif.model1 <- vif(model1 <- lm(ViolentCrimesPerPop ~ . -racepctblack - agePct12t29
-medIncome - pctWWage -PctPopUnderPov -MalePctNevMarr -PctIlleg
-RentLowQ -NumInShelters, communist_data))
model1
vif.model1[sqrt(vif.model1)>=2]
#cor(communist_data)
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.pos = 'H')
rm(list = ls())
library(car)
data("Chirot")
str(Chirot)
model <- lm(intensity ~ commerce + tradition + midpeasant + inequality, data =  Chirot)
summary(model)
plot(Chirot$intensity, Chirot$commerce)
plot(Chirot$intensity, Chirot$tradition)
plot(Chirot$intensity, Chirot$midpeasant)
plot(Chirot$intensity, Chirot$inequality)
rm(list = ls())
library(car)
data("Chirot")
str(Chirot)
model <- lm(intensity ~ commerce + tradition + midpeasant + inequality, data =  Chirot)
ddf <- data.frame(hatvalues(model), resid(model), rstandard(model), rstudent(model),
cooks.distance(model))
ddf
kable(ddf)
##  Obsservations with hat values greater than twice the hat bar.
ddf[ddf$hatvalues.model. > 2*((4+1)/nrow(Chirot)), ]
ddf[abs(ddf$rstandard) > qt((0.975), df = 6) | abs(ddf$rstudent)> qt((0.975), df = 6), ]
ddf[abs(ddf$rstandard) > qnorm((0.975)) | abs(ddf$rstudent)> qnorm((0.975)), ]
ddf[abs(ddf$rstandard) > qnorm((0.975)) | abs(ddf$rstudent)> qnorm((0.975)), ]
